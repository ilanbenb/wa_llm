{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic_ai import Agent\n",
    "import dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from whatstk import WhatsAppChat\n",
    "import uuid\n",
    "\n",
    "\n",
    "# dotenv.load_dotenv()\n",
    "\n",
    "# file_path = '../data/WhatsAppChat/_chat.txt'\n",
    "\n",
    "# wa_chat = WhatsAppChat.from_source(filepath=file_path)\n",
    "\n",
    "# file_path = '../data/WhatsAppChat/whatsmeow_contacts_202502090741.csv'\n",
    "# contacts_df = pd.read_csv(file_path)\n",
    "\n",
    "# def match_and_rename_users(wa_chat: WhatsAppChat, contacts_df: DataFrame) -> WhatsAppChat:\n",
    "#     dict_of_users = defaultdict(list)\n",
    "\n",
    "#     contacts_df.fillna(\"\", inplace=True)\n",
    "\n",
    "#     for index, row in contacts_df.iterrows():\n",
    "#         phone_number = row['their_jid'].split('@')[0]\n",
    "#         # Using standard hyphen and handling variable length numbers\n",
    "#         long_number = f'+{phone_number[0:3]} {phone_number[3:5]}-{phone_number[5:8]}-{phone_number[8:]}'\n",
    "#         dict_of_users[phone_number].extend([long_number])\n",
    "\n",
    "#         if row['full_name']:\n",
    "#             dict_of_users[phone_number].extend([\n",
    "#                 row['full_name'],\n",
    "#                 f\"~ {row['full_name']}\"\n",
    "#             ])\n",
    "\n",
    "#         elif row['push_name']:\n",
    "#             dict_of_users[phone_number].extend([\n",
    "#                 row['push_name'],\n",
    "#                 f\"~ {row['push_name']}\"\n",
    "#             ])\n",
    "            \n",
    "#     dict_of_users = {k: list(set(v)) for k, v in dict_of_users.items()}\n",
    "\n",
    "#     swapped_names = wa_chat.rename_users(mapping=dict_of_users)\n",
    "#     return swapped_names\n",
    "\n",
    "# # renames users to phone numbers for latter tagging use\n",
    "# renamed_wa_chat = match_and_rename_users(wa_chat, contacts_df)\n",
    "\n",
    "# chat_df = renamed_wa_chat.df\n",
    "\n",
    "# #filter out not interesting messages\n",
    "# patterns = '|'.join([\n",
    "# \"This message was deleted\",\n",
    "# \"you deleted this message.\",\n",
    "# \"you deleted this message as admin\",\n",
    "# \"Contact card omitted\",\n",
    "# \"GIF omitted\", \n",
    "# \"image omitted\",\n",
    "# \"video omitted\",\n",
    "#  r'\\d{3}[-‐]?\\d{3,4}\\s+left',  # matches phone number patterns followed by \"left\"\n",
    "# \"requested to join\",\n",
    "# \"Your security code with \",\n",
    "# 'security code',\n",
    "# 'pinned a message',\n",
    "# 'changed their phone number to a new number',\n",
    "# joined using this group\\'s invite link',\n",
    "# 'Waiting for this message'\n",
    "# ])\n",
    "\n",
    "# chat_df = chat_df[~chat_df['message'].str.contains(patterns, case=False, na=False, regex=True)]\n",
    "\n",
    "\n",
    "# chat_df['group'] = \"GenAI Israel\"\n",
    "# chat_df['id'] = [f\"imported_{uuid.uuid4()}\" for _ in range(len(chat_df))]\n",
    "\n",
    "chat_df_file_path = \"../data/WhatsAppChat/chat_df.csv\"\n",
    "chat_df = pd.read_csv(chat_df_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups chat to Conversations. From Conversations to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from models import Message\n",
    "from typing import Dict\n",
    "from daily_ingest.daily_ingest import get_conversation_topics\n",
    "from sqlalchemy.ext.asyncio import create_async_engine\n",
    "from sqlmodel.ext.asyncio.session import AsyncSession\n",
    "from sqlmodel import select\n",
    "from config import Settings\n",
    "from models.group import Group\n",
    "\n",
    "\n",
    "settings = Settings()  # pyright: ignore [reportCallIssue]\n",
    "\n",
    "engine = create_async_engine(\n",
    "    settings.db_uri,\n",
    "    pool_size=20,\n",
    "    max_overflow=40,\n",
    "    pool_timeout=30,\n",
    "    pool_pre_ping=True,\n",
    "    pool_recycle=600,\n",
    "    future=True,\n",
    ")\n",
    "db_session = AsyncSession(engine)\n",
    "\n",
    "def _identify_conversations(df, time_threshold_minutes):\n",
    "    \"\"\"\n",
    "    Identifies separate conversations in WhatsApp chat data based on time gaps between messages.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing WhatsApp chat data with 'date' column\n",
    "    time_threshold_minutes (int): Time gap (in minutes) to consider as a new conversation\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original DataFrame with an additional 'conversation_id' column\n",
    "    \"\"\"\n",
    "    # Make sure we're working with a copy\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert date column to datetime if it's not already\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Initialize conversation ID\n",
    "    conversation_id = 0\n",
    "    conversation_ids = [conversation_id]\n",
    "    \n",
    "    # Iterate through messages (except the first one)\n",
    "    for i in range(1, len(df)):\n",
    "        current_time = df.iloc[i]['date']\n",
    "        previous_time = df.iloc[i-1]['date']\n",
    "        \n",
    "        # Calculate time difference in minutes\n",
    "        time_diff = (current_time - previous_time).total_seconds() / 60\n",
    "        \n",
    "        # If time difference is greater than threshold, start new conversation\n",
    "        if time_diff > time_threshold_minutes:\n",
    "            conversation_id += 1\n",
    "            \n",
    "        conversation_ids.append(conversation_id)\n",
    "    \n",
    "    # Add conversation IDs to DataFrame\n",
    "    df['conversation_id'] = conversation_ids\n",
    "    return df\n",
    "\n",
    "def _create_user_mapping(wa_df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Creates a mapping of usernames to shortened names (@user_[id]),\n",
    "    where more frequent speakers get lower IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing WhatsApp chat data with 'username' column\n",
    "    \n",
    "    Returns:\n",
    "    Dict[str, str]: Mapping of original usernames to shortened names\n",
    "    \"\"\"\n",
    "    # Count messages per user and sort by frequency (descending)\n",
    "    user_counts = wa_df['username'].value_counts()\n",
    "    \n",
    "    # Create mapping with lower IDs for more frequent speakers\n",
    "    user_mapping = {username: f\"@user_{i+1}\" \n",
    "                   for i, (username, _) in enumerate(user_counts.items())}\n",
    "    \n",
    "    return user_mapping\n",
    "\n",
    "\"\"\"\n",
    "Provides summary statistics for each conversation, processing multiple conversations concurrently.\n",
    "\n",
    "Parameters:\n",
    "df (pandas.DataFrame): DataFrame with conversation_id column\n",
    "\n",
    "Returns:\n",
    "pandas.DataFrame: Summary statistics for each conversation\n",
    "\"\"\"\n",
    "async def _process_conversation(conv_id: int) -> Dict:\n",
    "    \"\"\"Helper function to process a single conversation\"\"\"\n",
    "    conv_data = df_with_conversation_ids[df_with_conversation_ids['conversation_id'] == conv_id]\n",
    "    messages = [Message(\n",
    "        message_id=f\"na-{row['date']}\",\n",
    "        timestamp=row['date'],\n",
    "        chat_jid=row['group'],\n",
    "        text=row['message'],\n",
    "        sender_jid=row['username'],\n",
    "        group_jid=row['group']\n",
    "    ) for _, row in conv_data.iterrows()]\n",
    "    topics = await get_conversation_topics(messages)\n",
    "    \n",
    "    return {\n",
    "        'conversation_id': conv_id,\n",
    "        'group_name': conv_data['group'].iat[0],\n",
    "        'start_time': conv_data['date'].min(),\n",
    "        'end_time': conv_data['date'].max(),\n",
    "        'duration_minutes': (conv_data['date'].max() - conv_data['date'].min()).total_seconds() / 60,\n",
    "        'num_messages': len(conv_data),\n",
    "        'num_participants': conv_data['username'].nunique(),\n",
    "        'participants': ', '.join(conv_data['username'].astype(str).unique()),\n",
    "        'conversation_content': conv_data,\n",
    "        'topics': topics,\n",
    "    }\n",
    "\n",
    "async def _bounded_process_conversation(conversation_id: int) -> Dict:\n",
    "    async with semaphore:\n",
    "        return await _process_conversation(conversation_id)\n",
    "\n",
    "user_mapping = _create_user_mapping(chat_df)\n",
    "\n",
    "# Add mapped usernames as a new column\n",
    "chat_df['mapped_username'] = chat_df['username'].map(user_mapping)\n",
    "\n",
    "group_from_file = (await db_session.exec(select(Group).where(Group.group_name == chat_df[1]['group']))).first()\n",
    "\n",
    "\n",
    "# Identify conversations\n",
    "df_with_conversation_ids = _identify_conversations(chat_df, time_threshold_minutes=60*3)\n",
    "\n",
    "conversation_ids = df_with_conversation_ids['conversation_id'].unique()\n",
    "\n",
    "# If you want to limit the number of conversations to process\n",
    "# conversation_ids = conversation_ids[:1000]\n",
    "\n",
    "# Create a semaphore to limit concurrency to max_concurrency tasks\n",
    "max_concurrency = 3\n",
    "semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "# Process all conversations concurrently, limited by the semaphore\n",
    "conversation_stats = await asyncio.gather(\n",
    "    *[_bounded_process_conversation(conversation_id) for conversation_id in conversation_ids]\n",
    ")  \n",
    "\n",
    "conversations = pd.DataFrame(conversation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all topics\n",
    "from voyageai.client_async import AsyncClient\n",
    "from models.knowledge_base_topic import KBTopic, KBTopicCreate\n",
    "from utils.voyage_embed_text import voyage_embed_text\n",
    "\n",
    "\n",
    "\n",
    "embedding_client = AsyncClient(\n",
    "            api_key=settings.voyage_api_key,\n",
    "            max_retries=settings.voyage_max_retries\n",
    "    )\n",
    "\n",
    "topics_df = pd.DataFrame(columns=['subject', 'summary','speakers','start_time', 'group', 'id'])\n",
    "\n",
    "\n",
    "for conversation in conversations.iterrows():\n",
    "    topics = conversation[1]['topics']\n",
    "    for topic in topics:\n",
    "        conversation_content = \"\\n\".join(\n",
    "            f\"{row['date']}: {[row['username']]}: {row['message']}\"\n",
    "            for _, row in conversation[1]['conversation_content'].iterrows()\n",
    "        )\n",
    "\n",
    "        # Get Group JID from conversation\n",
    "        messages: list[Message] = res.all()\n",
    "        new_row = pd.DataFrame([{\n",
    "\n",
    "               'id': f\"topic_{uuid.uuid4()}\",\n",
    "               'subject': topic.subject,\n",
    "               'summary': topic.summary,\n",
    "               'speakers': topic.speakers,\n",
    "               'start_time': conversation[1]['start_time'],\n",
    "               'original_conversation': conversation_content,\n",
    "                'group_jid': group_from_file.group_jid,\n",
    "        }])\n",
    "        topics_df = pd.concat([topics_df, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "documents = topics_df.apply(\n",
    "    lambda row: f\"# {row['subject']}\\n{row['summary']}\", axis=1\n",
    ").tolist()\n",
    "\n",
    "topics_embedings = await voyage_embed_text(embedding_client, documents)\n",
    "doc_models = [\n",
    "        KBTopicCreate(\n",
    "            #  TODO: decide on a meaningfull ID to allow upserts. Probably group_jid + subject and start_time to allow for multiple topics with the same subject\n",
    "            id=str(uuid.uuid4()),\n",
    "            embedding=emb,\n",
    "            group_jid=group_from_file.group_jid,\n",
    "            start_time=topic_row[\"start_time\"],\n",
    "            # TODO: migrate speakers to a list\n",
    "            speakers=topic_row[\"speakers\"],\n",
    "            summary=topic_row[\"summary\"],\n",
    "            subject=topic_row[\"subject\"]\n",
    "        ) # type: ignore\n",
    "        for topic_row, emb in zip(\n",
    "            topics_df,\n",
    "            topics_embedings\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# Once we give a meaningfull ID, we should migrate to upsert! \n",
    "db_session.add_all([KBTopic(**doc.model_dump()) for doc in doc_models])\n",
    "await db_session.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
